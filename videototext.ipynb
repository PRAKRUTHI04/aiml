{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"videos\"  # Folder containing videos\n",
    "output_dir = \"frames\"  # Folder to save extracted frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'videos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, video_file)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m video_file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mkv\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'videos'"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for video_file in os.listdir(video_dir):\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    if not video_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        print(f\"Skipping unsupported file: {video_file}\")\n",
    "        continue\n",
    "    video_name = os.path.splitext(video_file)[0]\n",
    "    video_frames_dir = os.path.join(output_dir, video_name)\n",
    "    os.makedirs(video_frames_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_file}\")\n",
    "        continue  \n",
    "    \n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_filename = os.path.join(video_frames_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clips: 2485\n",
      "Clip shape: (16, 64, 64, 3)\n",
      "Labels shape: (2485,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_clips(data_dir, clip_length=16, frame_size=(64,64)):\n",
    "    clips = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(data_dir))  # Get the list of class folders (e.g., class001, class002)\n",
    "    \n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):  # Skip if it's not a folder\n",
    "            continue\n",
    "            \n",
    "        # Loop through each subdirectory (representing a video) inside a class\n",
    "        for video_folder in os.listdir(class_dir):\n",
    "            video_path = os.path.join(class_dir, video_folder)\n",
    "            if not os.path.isdir(video_path):  # Ensure it's a directory\n",
    "                continue\n",
    "\n",
    "            # Check if there are nested subdirectories\n",
    "            nested_dirs = [d for d in os.listdir(video_path) if os.path.isdir(os.path.join(video_path, d))]\n",
    "            if nested_dirs:\n",
    "                # Assume frames are in the first nested subdirectory\n",
    "                video_path = os.path.join(video_path, nested_dirs[0])\n",
    "            \n",
    "            frames = sorted(os.listdir(video_path))  # Ensure frames are ordered\n",
    "            video_frames = []\n",
    "\n",
    "            for frame_file in frames:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                frame = cv2.imread(frame_path)\n",
    "                if frame is None:  # Skip invalid frames\n",
    "                    continue\n",
    "                frame = cv2.resize(frame, frame_size)  # Resize to the target size\n",
    "                video_frames.append(frame)\n",
    "            \n",
    "            # Break the video into clips of clip_length\n",
    "            for i in range(0, len(video_frames) - clip_length + 1, clip_length):\n",
    "                clip = video_frames[i:i+clip_length]\n",
    "                clips.append(np.array(clip))\n",
    "                labels.append(label)\n",
    "    \n",
    "    return np.array(clips), np.array(labels)\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"video_to_frames\"  # Root directory containing classXXX folders\n",
    "X_data, y_data = load_clips(data_dir)\n",
    "\n",
    "print(\"Number of clips:\", len(X_data))\n",
    "print(\"Clip shape:\", X_data[0].shape if len(X_data) > 0 else \"No clips loaded\")\n",
    "print(\"Labels shape:\", y_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_8 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m) │         \u001b[38;5;34m2,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_8 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_9 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m55,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_9 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m4,644\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,274,020</span> (12.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,274,020\u001b[0m (12.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,274,020</span> (12.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,274,020\u001b[0m (12.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "class_names = sorted(os.listdir(data_dir)) \n",
    "model = Sequential([\n",
    "    Conv3D(32, (3, 3, 3), activation='relu', input_shape=(16, 64, 64, 3)),\n",
    "    MaxPooling3D((2, 2, 2)),\n",
    "    Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D((2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(class_names), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n",
      "Epoch 1/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 221ms/step - accuracy: 0.0564 - loss: 3.6550 - val_accuracy: 0.2460 - val_loss: 2.6012\n",
      "Epoch 2/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 174ms/step - accuracy: 0.2816 - loss: 2.5434 - val_accuracy: 0.5927 - val_loss: 1.4530\n",
      "Epoch 3/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 177ms/step - accuracy: 0.5106 - loss: 1.6677 - val_accuracy: 0.6492 - val_loss: 1.1905\n",
      "Epoch 4/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 172ms/step - accuracy: 0.5969 - loss: 1.3127 - val_accuracy: 0.7177 - val_loss: 0.8504\n",
      "Epoch 5/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - accuracy: 0.6983 - loss: 0.9939 - val_accuracy: 0.7742 - val_loss: 0.6929\n",
      "Epoch 6/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 192ms/step - accuracy: 0.7469 - loss: 0.7798 - val_accuracy: 0.8105 - val_loss: 0.5416\n",
      "Epoch 7/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 186ms/step - accuracy: 0.7707 - loss: 0.6654 - val_accuracy: 0.8790 - val_loss: 0.4346\n",
      "Epoch 8/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 191ms/step - accuracy: 0.8178 - loss: 0.5144 - val_accuracy: 0.8992 - val_loss: 0.3743\n",
      "Epoch 9/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 205ms/step - accuracy: 0.8516 - loss: 0.4604 - val_accuracy: 0.8629 - val_loss: 0.4001\n",
      "Epoch 10/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 194ms/step - accuracy: 0.8720 - loss: 0.3957 - val_accuracy: 0.8871 - val_loss: 0.3538\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - accuracy: 0.8709 - loss: 0.4651\n",
      "Test Loss: 0.4415052831172943\n",
      "Test Accuracy: 0.8755019903182983\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"video_to_frames\"  # Root directory containing classXXX folders\n",
    "x, y = load_clips(data_dir)\n",
    "x = x.astype(np.float32)\n",
    "x = x.astype(np.float32)\n",
    "# Assuming you already have your dataset: x_train, y_train, x_val, y_val, x_test, y_test\n",
    "x_train, x_remaining, y_train, y_remaining = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_remaining, y_remaining, test_size=0.5, random_state=100)\n",
    "\n",
    "# Normalize the data (if not already done)\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "\n",
    "x_val = x_val.astype(np.float32)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 36  # Change this to the number of classes in your dataset\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "depth = 16  # Number of frames in each clip\n",
    "height = 64  # Height of each frame\n",
    "width = 64  # Width of each frame\n",
    "channels = 3  # RGB channels\n",
    "# Define your 3D CNN model (replace this with your model if already defined)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(depth, height, width, channels)),\n",
    "    tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10,  # Adjust based on your needs\n",
    "    batch_size=8,  # Adjust based on available GPU memory\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"sign_language_model.h5\")\n",
    "loaded_model = tf.keras.models.load_model(\"sign_language_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to stop capturing frames...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m frames_queue \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39mdepth)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"sign_language_model.h5\")\n",
    "\n",
    "# Parameters\n",
    "depth = 16  # Number of frames per clip\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "frames_dir = \"webcam_frames\"  # Directory to store captured frames\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "# Automatically generate class labels based on the model's structure or pre-defined mapping\n",
    "class_labels = {idx: f\"class{str(idx).zfill(3)}\" for idx in range(36)}  # Adjust based on the number of classes\n",
    "\n",
    "# Step 1: Capture video and save frames\n",
    "print(\"Press 'q' to stop capturing frames...\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "frames_queue = deque(maxlen=depth)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save frame to the specified directory\n",
    "    frame_path = os.path.join(frames_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    frame_count += 1\n",
    "\n",
    "    # Display the webcam feed\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "    # Exit capturing loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Captured {frame_count} frames to '{frames_dir}'.\")\n",
    "\n",
    "# Step 2: Load frames for prediction\n",
    "def load_frames_for_prediction(frames_dir, frame_size=(height, width)):\n",
    "    \"\"\"\n",
    "    Load frames from the directory and preprocess them for model prediction.\n",
    "    \"\"\"\n",
    "    frame_files = sorted(os.listdir(frames_dir))  # Ensure frames are in correct order\n",
    "    frames = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is not None:\n",
    "            frame = cv2.resize(frame, frame_size) / 255.0  # Resize and normalize\n",
    "            frames.append(frame)\n",
    "    return np.array(frames)\n",
    "\n",
    "# Load frames and group them into clips of 'depth' size\n",
    "frames = load_frames_for_prediction(frames_dir)\n",
    "num_clips = len(frames) // depth\n",
    "clips = [frames[i * depth:(i + 1) * depth] for i in range(num_clips)]\n",
    "\n",
    "# Step 3: Predict signs from frames\n",
    "predicted_labels = []\n",
    "for clip_idx, clip in enumerate(clips):\n",
    "    if len(clip) == depth:\n",
    "        input_clip = np.expand_dims(clip, axis=0)  # (1, depth, height, width, channels)\n",
    "        predictions = model.predict(input_clip)\n",
    "        predicted_class = np.argmax(predictions[0])  # Get the class index\n",
    "        predicted_label = class_labels[predicted_class]  # Get the corresponding label\n",
    "        predicted_labels.append(predicted_label)\n",
    "        print(f\"Clip {clip_idx + 1}: Predicted - {predicted_label}\")\n",
    "\n",
    "# Step 4: Annotate frames with predictions\n",
    "annotated_dir = \"annotated_frames\"\n",
    "os.makedirs(annotated_dir, exist_ok=True)\n",
    "for i, frame_file in enumerate(sorted(os.listdir(frames_dir))[:len(predicted_labels) * depth]):\n",
    "    frame_path = os.path.join(frames_dir, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    clip_idx = i // depth\n",
    "    if clip_idx < len(predicted_labels):\n",
    "        predicted_label = predicted_labels[clip_idx]\n",
    "        cv2.putText(frame, f\"Predicted: {predicted_label}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    annotated_path = os.path.join(annotated_dir, f\"frame_{i:04d}.jpg\")\n",
    "    cv2.imwrite(annotated_path, frame)\n",
    "\n",
    "print(f\"Annotated frames saved to '{annotated_dir}'.\")\n",
    "\n",
    "# Optional: Combine annotated frames into a video\n",
    "output_video_path = \"output_video.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for MP4\n",
    "fps = 30  # Set frames per second\n",
    "frame_size = (frame.shape[1], frame.shape[0])  # Frame size from the first frame\n",
    "\n",
    "video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "for frame_file in sorted(os.listdir(annotated_dir)):\n",
    "    frame_path = os.path.join(annotated_dir, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is not None:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Annotated video saved to '{output_video_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 337ms/step - accuracy: 0.8680 - loss: 0.3872 - precision: 0.9040 - recall: 0.8456 - val_accuracy: 0.8871 - val_loss: 0.3364 - val_precision: 0.9076 - val_recall: 0.8710\n",
      "Epoch 2/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.8829 - loss: 0.3396 - precision: 0.9153 - recall: 0.8595 - val_accuracy: 0.9073 - val_loss: 0.3413 - val_precision: 0.9310 - val_recall: 0.8710\n",
      "Epoch 3/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 282ms/step - accuracy: 0.9007 - loss: 0.3181 - precision: 0.9286 - recall: 0.8711 - val_accuracy: 0.8992 - val_loss: 0.3452 - val_precision: 0.9234 - val_recall: 0.8750\n",
      "Epoch 4/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.9010 - loss: 0.3319 - precision: 0.9285 - recall: 0.8631 - val_accuracy: 0.8911 - val_loss: 0.3246 - val_precision: 0.8963 - val_recall: 0.8710\n",
      "Epoch 5/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 264ms/step - accuracy: 0.8981 - loss: 0.3125 - precision: 0.9298 - recall: 0.8719 - val_accuracy: 0.9113 - val_loss: 0.3149 - val_precision: 0.9328 - val_recall: 0.8952\n",
      "Epoch 6/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 265ms/step - accuracy: 0.9010 - loss: 0.2612 - precision: 0.9400 - recall: 0.8818 - val_accuracy: 0.9113 - val_loss: 0.3047 - val_precision: 0.9375 - val_recall: 0.9073\n",
      "Epoch 7/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 280ms/step - accuracy: 0.9052 - loss: 0.2641 - precision: 0.9273 - recall: 0.8815 - val_accuracy: 0.9073 - val_loss: 0.3381 - val_precision: 0.9184 - val_recall: 0.9073\n",
      "Epoch 8/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 296ms/step - accuracy: 0.9187 - loss: 0.2106 - precision: 0.9396 - recall: 0.9090 - val_accuracy: 0.9194 - val_loss: 0.2603 - val_precision: 0.9262 - val_recall: 0.9113\n",
      "Epoch 9/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 264ms/step - accuracy: 0.9091 - loss: 0.2565 - precision: 0.9256 - recall: 0.8867 - val_accuracy: 0.9153 - val_loss: 0.2560 - val_precision: 0.9298 - val_recall: 0.9073\n",
      "Epoch 10/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 258ms/step - accuracy: 0.9256 - loss: 0.2216 - precision: 0.9407 - recall: 0.9070 - val_accuracy: 0.9355 - val_loss: 0.1922 - val_precision: 0.9429 - val_recall: 0.9315\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.9413 - loss: 0.2679 - precision: 0.9520 - recall: 0.9278\n",
      "Test Loss: 0.26422011852264404\n",
      "Test Accuracy: 0.9437751173973083\n",
      "Test Precision: 0.9508196711540222\n",
      "Test Recall: 0.9317269325256348\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Compile the model with additional metrics\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10,  # Adjust based on your needs\n",
    "    batch_size=8,  # Adjust based on available GPU memory\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m {idx: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m36\u001b[39m)}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Start video capture\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"sign_language_model.h5\")\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Parameters\n",
    "depth = 16  # Frames per clip\n",
    "height, width, channels = 64, 64, 3\n",
    "frames_queue = deque(maxlen=depth)\n",
    "class_labels = {idx: f\"class{str(idx).zfill(3)}\" for idx in range(36)}\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess frame: resize and normalize\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    frames_queue.append(normalized_frame)\n",
    "\n",
    "    # Debug input shapes\n",
    "    if len(frames_queue) == depth:\n",
    "        input_clip = np.expand_dims(np.array(frames_queue), axis=0)\n",
    "        print(f\"Input clip shape: {input_clip.shape}\")  # Debug print\n",
    "        \n",
    "        # Make predictions\n",
    "        try:\n",
    "            predictions = model.predict(input_clip)\n",
    "            predicted_class = np.argmax(predictions[0])\n",
    "            predicted_label = class_labels[predicted_class]\n",
    "\n",
    "            # Display prediction\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_label}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "    # Quit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esumm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
